{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cargamos el data set\n",
    "import pandas as pd\n",
    "ds = pd.read_csv('twitter_gender_50k.csv', sep='\\n', names= [\"text\"])\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,one person followed me and 2 people unfollowed me \\\\/\\\\/ automatically checked by https:\\\\/\\\\/t.co\\\\/qsyIbIajjh'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet = []\n",
    "for i in ds['text']:\n",
    "    tweet.append(i[2:])\n",
    "\n",
    "male_female = []\n",
    "for i in ds['text']:\n",
    "    male_female.append(i[:1])\n",
    "    \n",
    "data = pd.DataFrame(\n",
    "    {'male_female': male_female,\n",
    "     'tweet': tweet,\n",
    "    })\n",
    "##data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male_female</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>one person followed me and 2 people unfollowed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BABY https:\\/\\/t.co\\/r0R86TBLIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#FairMediaReportingForJaDine Jadines For JaDin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Grade 2 learning about erosion https:\\/\\/t.co\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@UnbridledShrewd Hello Klaus. *She said leanin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  male_female                                              tweet\n",
       "0           1  one person followed me and 2 people unfollowed...\n",
       "1           1                    BABY https:\\/\\/t.co\\/r0R86TBLIF\n",
       "2           1  #FairMediaReportingForJaDine Jadines For JaDin...\n",
       "3           1  Grade 2 learning about erosion https:\\/\\/t.co\\...\n",
       "4           1  @UnbridledShrewd Hello Klaus. *She said leanin..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'one person followed me and 2 people unfollowed me \\\\/\\\\/ automatically checked by https:\\\\/\\\\/t.co\\\\/qsyIbIajjh',\n",
       "       'BABY https:\\\\/\\\\/t.co\\\\/r0R86TBLIF',\n",
       "       \"#FairMediaReportingForJaDine Jadines For JaDine If you're no good at all in your job\",\n",
       "       ...,\n",
       "       '#branding this should surely interest you https:\\\\/\\\\/t.co\\\\/OaQVtle8Az https:\\\\/\\\\/t.co\\\\/Jfz1hzY5z5',\n",
       "       'Never be afraid to fail. https:\\\\/\\\\/t.co\\\\/dtC0jYzo3v https:\\\\/\\\\/t.co\\\\/9ErrXM61i8',\n",
       "       '@paulhenson19 @jonmachota Yep. Apples and oranges.'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "documents = np.asarray(data['tweet'])\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "### Limpiamos la data\n",
    "doc_clear_processed = []\n",
    "for item in documents:\n",
    "    newitem = re.sub(\"@[A-Za-z0-9_]+\",'',item) \n",
    "    newitem = re.sub(\"#[A-Za-z0-9_]+\",'',newitem) \n",
    "    newitem = re.sub('\\\\\\\\','',newitem)\n",
    "    newitem = re.sub(\"https://[A-Za-z0-9-//.]+\",'',newitem)\n",
    "    newitem = re.sub(\"[.,_-]+\",' ',newitem)\n",
    "    newitem = re.sub(\"[^A-Za-z0-9]+$\",'',newitem)\n",
    "    newitem = re.sub(\"^[^A-Za-z0-9]+\",'',newitem)\n",
    "    newitem = re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',newitem)\n",
    "    newitem = re.sub(\"[ ]+\",' ',newitem)\n",
    "    doc_clear_processed.append(newitem)\n",
    "#exportamos a un archivo de textto para mayor visualizacion\n",
    "with open(\"file.txt\", \"w\",encoding='utf-8') as output:\n",
    "    for item in doc_clear_processed:\n",
    "        print(item, end='\\n', file=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4447029"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### separamos los tweets en tokens\n",
    "tokens = []\n",
    "for d in doc_clear_processed:\n",
    "    tokens.extend(d.split())\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count_t = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173581"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 110580),\n",
       " ('to', 100262),\n",
       " ('a', 88536),\n",
       " ('I', 81261),\n",
       " ('and', 62110),\n",
       " ('you', 55382),\n",
       " ('in', 54751),\n",
       " ('of', 54343),\n",
       " ('for', 50486),\n",
       " ('is', 48090),\n",
       " ('on', 35774),\n",
       " ('my', 31036),\n",
       " ('it', 29726),\n",
       " ('me', 29638),\n",
       " ('with', 27539)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###las palabras mas repetida\n",
    "count_t.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_preprocessed = []\n",
    "doc_tokens = []\n",
    "for d in doc_clear_processed:\n",
    "    d = d.lower()\n",
    "    doc_preprocessed.append(d)\n",
    "    t = nltk.word_tokenize(d)\n",
    "    doc_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inside graceland nancy rooks if you wanted a picture of the life of elvis presley who better to paint it for'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preprocessed[951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inside',\n",
       " 'graceland',\n",
       " 'nancy',\n",
       " 'rooks',\n",
       " 'if',\n",
       " 'you',\n",
       " 'wanted',\n",
       " 'a',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'life',\n",
       " 'of',\n",
       " 'elvis',\n",
       " 'presley',\n",
       " 'who',\n",
       " 'better',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'it',\n",
       " 'for']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens[951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Procesamos los stopword\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remover los stopwords de cada elemento\n",
    "stopWords = set(stopwords.words('english'))\n",
    "doc_nostopwords = []\n",
    "sinRemover = 0\n",
    "for item in doc_tokens:\n",
    "    temp = []\n",
    "    for word in item:\n",
    "        if word not in stopWords:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            sinRemover = sinRemover +1\n",
    "    doc_nostopwords.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inside',\n",
       " 'graceland',\n",
       " 'nancy',\n",
       " 'rooks',\n",
       " 'wanted',\n",
       " 'picture',\n",
       " 'life',\n",
       " 'elvis',\n",
       " 'presley',\n",
       " 'better',\n",
       " 'paint']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_nostopwords[951]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inside',\n",
       " 'graceland',\n",
       " 'nancy',\n",
       " 'rook',\n",
       " 'wanted',\n",
       " 'picture',\n",
       " 'life',\n",
       " 'elvis',\n",
       " 'presley',\n",
       " 'better',\n",
       " 'paint']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "data_lem = copy.deepcopy(doc_nostopwords)\n",
    "#English\n",
    "from nltk.stem.porter import *\n",
    "ps = PorterStemmer()\n",
    "\n",
    "doc_lemmatizer = []\n",
    "for item in data_lem:\n",
    "    temp = []\n",
    "    for w in item:\n",
    "        temp.append(lemmatizer.lemmatize(w))\n",
    "    doc_lemmatizer.append(temp)\n",
    "doc_lemmatizer[951]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insid',\n",
       " 'graceland',\n",
       " 'nanci',\n",
       " 'rook',\n",
       " 'want',\n",
       " 'pictur',\n",
       " 'life',\n",
       " 'elvi',\n",
       " 'presley',\n",
       " 'better',\n",
       " 'paint']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stem = copy.deepcopy(doc_nostopwords)\n",
    "#English\n",
    "from nltk.stem.porter import *\n",
    "ps = PorterStemmer()\n",
    "\n",
    "doc_stemmer = []\n",
    "for item in data_stem:\n",
    "    temp = []\n",
    "    for w in item:\n",
    "        temp.append(ps.stem(w))\n",
    "    doc_stemmer.append(temp)\n",
    "doc_stemmer[951]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectores Característicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inside graceland nancy rook wanted picture life elvis presley better paint'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_lem = []\n",
    "for item in doc_lemmatizer:\n",
    "    documents_lem.append(' '.join(item))\n",
    "documents_lem[951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insid graceland nanci rook want pictur life elvi presley better paint'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_stem = []\n",
    "for item in doc_stemmer:\n",
    "    documents_stem.append(' '.join(item))\n",
    "documents_stem[951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100057)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Con stem\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(documents_stem)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 117819)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Con lem\n",
    "tfidf = TfidfVectorizer()\n",
    "X1 = tfidf.fit_transform(documents_lem)\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.asarray(data.male_female.transpose())\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Algoritmos de clasificacion \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_models(X, y):\n",
    "    run_model(LinearSVC(), X, y)\n",
    "    run_model(SGDClassifier(), X, y)\n",
    "    run_model(Perceptron(), X, y)\n",
    "    run_model(PassiveAggressiveClassifier(), X, y)\n",
    "    run_model(BernoulliNB(), X, y)\n",
    "    run_model(MultinomialNB(), X, y)\n",
    "    run_model(KNeighborsClassifier(), X, y)\n",
    "    run_model(NearestCentroid(), X, y)\n",
    "    run_model(RandomForestClassifier(n_estimators=100, max_depth=10), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 0.63 (+/- 0.01)\n",
      "SGDClassifier accuracy: 0.63 (+/- 0.01)\n",
      "Perceptron accuracy: 0.57 (+/- 0.01)\n",
      "PassiveAggressiveClassifier accuracy: 0.60 (+/- 0.02)\n",
      "BernoulliNB accuracy: 0.63 (+/- 0.01)\n",
      "MultinomialNB accuracy: 0.63 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "run_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_models(X1, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
